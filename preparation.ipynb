{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.phi3_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pdf documents\n",
    "documents_1 = ''\n",
    "\n",
    "reader = PdfReader('/workspaces/digital_research_guide/Novel_Encoder_Training_for_Neural_Machine_Translation_in_Low_Resource_Settings (1).pdf')\n",
    "for page in reader.pages:\n",
    "    documents_1 += page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Novel Encoder Training for Neural Machine Translation\n",
      "in Low Resource Settings\n",
      "Anonymous ACL submission\n",
      "Abstract\n",
      "Large language models are pre-trained on large 001\n",
      "datasets to boost performance in downstream 002\n",
      "tasks. However, for Neural Machine Trans- 003\n",
      "lation (NMT), Encoder-Decoder models are 004\n",
      "used. Unlike Encoder-only or Decoder-only 005\n",
      "models, pretraining Encoder-Decoder models 006\n",
      "are not straightforward due to cross-attention 007\n",
      "between the Encoder and the Decoder blocks. 008\n",
      "Modification of the Encoder-Decoder model 009\n",
      "for pre-training introduces extra model com- 010\n",
      "plexity and increased training time. For low- 011\n",
      "resource datasets, the concept of pre-training 012\n",
      "is, thus, typically not applicable. To alleviate 013\n",
      "that, in this paper, we introduce a separate En- 014\n",
      "coder training step and show that our proposed 015\n",
      "method increases the performance of Neural 016\n",
      "Machine Translation (NMT) at low-resource 017\n",
      "settings. Our proposed method does not in- 018\n",
      "crease the model complexity and the increase 019\n",
      "in training time is low. We also show that using 020\n",
      "our Encoder training method results in better 021\n",
      "NMT accuracy measures such as BLEU. 022\n",
      "1 Introduction 023\n",
      "Transformer models are the most widely used ar- 024\n",
      "chitecture with their applications in almost all of 025\n",
      "the advanced NLP tools that are in use (Vaswani 026\n",
      "et al., 2017). For the Neural Machine Translation 027\n",
      "(NMT) task, transformers are the state-of-the-art as 028\n",
      "well. In NMT, the task is to translate a given source 029\n",
      "language to a specific target language, for which 030\n",
      "the Encoder-Decoder models or more specifically, 031\n",
      "the sequence-to-sequence models, are used. The 032\n",
      "Encoder is used to encode the source language and 033\n",
      "the Decoder produces the target language output 034\n",
      "with the help of encoded output from the Encoder. 035\n",
      "With the introduction of the concept of pre- 036\n",
      "training (Devlin et al., 2018), it has become the 037\n",
      "normal trend to pre-train Large Language Models 038\n",
      "(LLMs) and then fine-tune on specific downstream 039\n",
      "tasks. This approach provides a significant reduc- 040\n",
      "tion in development (training) time as well as an 041improvement in the performance of downstream 042\n",
      "tasks. The pre-training concept is easily applica- 043\n",
      "ble to Encoder-only models involving tasks such 044\n",
      "as sentence classification and named entity recog- 045\n",
      "nition, while Decoder-only models are useful for 046\n",
      "generative tasks such as text generation. However, 047\n",
      "the pre-training is not straightforward in NMT due 048\n",
      "to existence of cross-attention layers between the 049\n",
      "Encoder and the Decoder blocks. For the very same 050\n",
      "reason, we can not directly pre-train the Encoder 051\n",
      "and Decoder separately on the source and target 052\n",
      "language respectively. 053\n",
      "The concept of pre-training is possible to be 054\n",
      "applied for NMT but it involves the introduction 055\n",
      "of extra modules on top of the standard baseline 056\n",
      "Encoder-Decoder model as proposed in (Ren et al., 057\n",
      "2021). This comes at the cost of extra training time 058\n",
      "and model complexity. Pre-training also requires 059\n",
      "a parallel corpus where a passage in a source lan- 060\n",
      "guage has an exact correspondence in the target 061\n",
      "language. Large models require a large parallel 062\n",
      "dataset and, thus, there exists the problem of low- 063\n",
      "resource languages where such parallel corpora 064\n",
      "are not available. Hence, pre-training a model for 065\n",
      "NMT in low-resource settings is not applicable. 066\n",
      "Contributions 067\n",
      "In this paper, we try to integrate the advantage 068\n",
      "of pre-training an Encoder for NMT tasks at low- 069\n",
      "resource settings without significant additions to 070\n",
      "training time or model complexity. We show that 071\n",
      "by training the Encoder simply by using an extra 072\n",
      "head to produce the source tokens from the En- 073\n",
      "coder output, there is substantial improvement in 074\n",
      "NMT performance at low-resource settings. This 075\n",
      "approach is a part of the normal training procedure 076\n",
      "and does not require much change in the training 077\n",
      "procedure. Consequently, model complexity does 078\n",
      "not increase as well. 079\n",
      "We show that our approach gives improvement 080\n",
      "over baseline results and also faster convergence to 081\n",
      "1baseline results. The improvement in performance 082\n",
      "over the baseline comes at a cost of a very low 083\n",
      "increase in training time and complexity. 084\n",
      "The outline of the rest of the paper is as follows. 085\n",
      "In §2, we provide an overview of related works on 086\n",
      "this topic. Then, in §3, we give a brief description 087\n",
      "of the NMT architecture. In §4, we explain the En- 088\n",
      "coder training concept and how it can be integrated 089\n",
      "with our proposed Encoder training step. In §5, 090\n",
      "we describe the datasets used in our experiments 091\n",
      "before presenting the experimental results in §6. 092\n",
      "Finally, we conclude in §7. 093\n",
      "2 Related Work 094\n",
      "The end-to-end Neural Machine Translation (NMT) 095\n",
      "task was first introduced by Google (Wu et al., 096\n",
      "2016). From there onwards there has rapid devel- 097\n",
      "opment of new architectures and concepts such 098\n",
      "as attention mechanism and the introduction of 099\n",
      "the Transformer model by Google (Vaswani et al., 100\n",
      "2017). Yang et al. (2020) have performed a detailed 101\n",
      "study of the origin and principal development time- 102\n",
      "line of NMT. There has been a lot of work on pre- 103\n",
      "training Encoder-Decoder models for NMT tasks 104\n",
      "showing improvement in translation. Simultane- 105\n",
      "ously researchers also contributed to work on trans- 106\n",
      "lation in low resource settings. Ren et al. (2021) 107\n",
      "has proposed a pre-training method for NMT by 108\n",
      "introducing a semantic interface between Encoder 109\n",
      "and Decoder that allows them to communicate ef- 110\n",
      "fectively during pre-training. Guo et al. (2020) 111\n",
      "have proposed two different BERT models, one 112\n",
      "as the Encoder and the other as the Decoder and 113\n",
      "introduced adapter modules between BERT layers 114\n",
      "to use BERT for machine translation. It has shown 115\n",
      "translation results comparable to state-of-the-art 116\n",
      "models. In (Lin et al., 2020), the authors proposed 117\n",
      "a novel concept called mRASP Approach. It uses 118\n",
      "a technique called random aligned substitution. It 119\n",
      "involves pre-training a universal multilingual neu- 120\n",
      "ral machine translation model and then fine-tuning 121\n",
      "it on a specific language pair. Liu et al. (2020) 122\n",
      "proposed mBART which involves pre-training on 123\n",
      "large-scale monolingual corpora in multiple lan- 124\n",
      "guages using the BART architecture (Lewis et al., 125\n",
      "2019). The models show significant improvement 126\n",
      "in low-resource translation. 127\n",
      "3 NMT Background 128\n",
      "As mentioned in §1, the task of Neural Machine 129\n",
      "Translation (NMT) is to translate a passage of 130text in a given source language to a specific tar- 131\n",
      "get language. As the task involves understanding 132\n",
      "of source language and then producing the target 133\n",
      "language we use Encoder-Decoder or sequence-to- 134\n",
      "sequence models. The Encoder receives an input 135\n",
      "and builds a representation of it (essentially a form 136\n",
      "of its features). The Decoder then uses the En- 137\n",
      "coder’s representation (features) along with other 138\n",
      "inputs to generate a target sequence. Thus, the En- 139\n",
      "coder should be optimized to acquire understanding 140\n",
      "from the input, while the Decoder should be op- 141\n",
      "timized to generate outputs. In a standard NMT 142\n",
      "setting, the training of Encoder and Decoder is con- 143\n",
      "ducted as a single end-to-end system. 144\n",
      "The processing in the NMT model can be de- 145\n",
      "scribed in the following steps. The Encoder first 146\n",
      "encodes the source sentence xof length nconsist- 147\n",
      "ing of tokens x1, x2, . . . , x nand produces a repre- 148\n",
      "sentation xENCfor the same. After this step, the 149\n",
      "Decoder comes into action. The Decoder takes 150\n",
      "the Encoder output xENCand the previous target 151\n",
      "token as input and generates the next target token 152\n",
      "as output in sequence. For the very first target 153\n",
      "token generation, the 〈start-of-sentence 〉token is 154\n",
      "fed as the previous token. The Decoder produces 155\n",
      "a sequence yof length mconsisting of the out- 156\n",
      "put tokens y1, y2, . . . , y msequentially in an auto- 157\n",
      "regressive manner. The process stops when the 158\n",
      "Decoder outputs the 〈end-of-sentence 〉token, indi- 159\n",
      "cating the end of a sentence, or when a maximum 160\n",
      "number of token lengths is reached. 161\n",
      "wThe MLE training objective is defined as 162\n",
      "LMLE = Σm\n",
      "t=1logp(yt|yt−1, . . . , y 1, xENC) 163\n",
      "where tdenotes the time step and mis the number 164\n",
      "of tokens generated by the Decoder. The Trans- 165\n",
      "former processes input sequences in parallel during 166\n",
      "training, thereby significantly reducing computa- 167\n",
      "tion time. 168\n",
      "4 Encoder Training 169\n",
      "Before we dive into the training procedure for En- 170\n",
      "coder, let us first describe the inputs to Encoder 171\n",
      "and outputs from Encoder in detail. The input to 172\n",
      "the Encoder block is the tokenized source language 173\n",
      "sentence. Let us take the input size as seq_len 174\n",
      "where seq_len is the number of tokens that are in- 175\n",
      "put to the Encoder block. The length of output will 176\n",
      "be the same as the length of input. This is the rep- 177\n",
      "resentation of the input in a predefined embedding 178\n",
      "dimension space which is 512 in our case. Thus the 179\n",
      "2Encoder output size is (seq_len, embedding_dim). 180\n",
      "This representation goes as input to the Decoder 181\n",
      "block to generate the target language tokens. The 182\n",
      "intuition is that a good feature representation of 183\n",
      "input tokens in a different embedding dimension 184\n",
      "should also be able to produce back the original 185\n",
      "input tokens along with aiding the Decoder block 186\n",
      "to produce target tokens. 187\n",
      "We have added a head (encoder_head) on top of 188\n",
      "Encoder block comprising a linear layer with an 189\n",
      "input dimension equal to the embedding dimension 190\n",
      "and an output size equal to the source vocabulary 191\n",
      "size. The Encoder block output is passed through 192\n",
      "this layer to create a probability distribution of 193\n",
      "source vocabulary tokens for the entire sequence 194\n",
      "of Encoder output. The Encoder is trained with the 195\n",
      "objective to reproduce correct source tokens from 196\n",
      "the Encoder output which is also the input to the 197\n",
      "Decoder block. Thus, we now have a different loss 198\n",
      "function for the Encoder block. It is easy to train 199\n",
      "the Encoder with the above objective. Figure 1 in 200\n",
      "Appendix shows the architecture of our proposed 201\n",
      "Encoder training. 202\n",
      "Along with training the Encoder separately as 203\n",
      "described above we also train the entire Encoder- 204\n",
      "Decoder block as a single end-to-end system with 205\n",
      "NMT objective as described in §3. The training 206\n",
      "objective for the Encoder alone does not impart 207\n",
      "any inherent meaning but when used in conjunction 208\n",
      "with the end-to-end training objective of the NMT 209\n",
      "model, it helps in optimizing the Encoder for better 210\n",
      "feature representation in translation tasks. This 211\n",
      "encoder_head layer is used only during training. 212\n",
      "This layer is not involved in the inference. 213\n",
      "The above training objective for the Encoder 214\n",
      "block introduces two problems. Firstly, training the 215\n",
      "Encoder block separately with different objectives 216\n",
      "will introduce extra computation, thereby increas- 217\n",
      "ing the training time. Secondly, if we keep training 218\n",
      "the Encoder at every training step with the above 219\n",
      "mentioned objective, then initially model perfor- 220\n",
      "mance may improve but gradually it will subside 221\n",
      "as the Encoder training objective is to optimize 222\n",
      "the Encoder output to reproduce the Encoder input 223\n",
      "tokens and not the target tokens. 224\n",
      "To handle the above problems, we made some 225\n",
      "changes to the training procedure. We divided the 226\n",
      "training step into two categories. In Category I, 227\n",
      "we train the model with both the training objec- 228\n",
      "tives, i.e., Encoder training objective as well as the 229\n",
      "end-to-end training objective. In Category II, we 230\n",
      "train the model with only the end-to-end training 231objective. At every training step, we pick one of 232\n",
      "the categories for training with a given probability 233\n",
      "distribution. At the beginning of every epoch, we 234\n",
      "pick both the categories with equal probability. The 235\n",
      "probability of choosing Category I or Category II 236\n",
      "is updated after every 1000 training steps. After 237\n",
      "every 1000 training steps, we set the probability of 238\n",
      "Category I equal to the proportion of source tokens 239\n",
      "predicted wrong for Encoder training during these 240\n",
      "1000 training steps. The probability of Category II 241\n",
      "is consequently set to one minus the probability for 242\n",
      "Category I. 243\n",
      "With the above training approach, initially, the 244\n",
      "Category I probability is very high, but as training 245\n",
      "progresses, the probability goes down. Since with 246\n",
      "every training step, typically the number of errors 247\n",
      "and, thus, the probability of category I decreases, 248\n",
      "the number of times Encoder training is executed 249\n",
      "also decreases. Thus, the extra computation time 250\n",
      "required for Encoder training is reduced. This also 251\n",
      "ensures that the Encoder training objective is not 252\n",
      "executed any more than required. This prevents 253\n",
      "the model performance from subsiding because of 254\n",
      "excess separate Encoder training whose objective 255\n",
      "is different from the NMT objective. This approach 256\n",
      "eliminates the problems for separate Encoder train- 257\n",
      "ing objectives while retaining its benefits. 258\n",
      "5 Datasets 259\n",
      "We conducted experiments on four different 260\n",
      "datasets. One dataset is kde4 and rest of the 261\n",
      "datasets are from Helsinki-NLP/opus-100 (Zhang 262\n",
      "et al., 2020; Tiedemann, 2012). The datasets used 263\n",
      "are as follows: 264\n",
      "•kde4 en-fr subset : English-French parallel 265\n",
      "corpus. Training set size 178K. Source and 266\n",
      "Target vocabulary size is 30K each. 267\n",
      "•Helsinki-NLP/opus-100 af-en subset : 268\n",
      "Afrikaans-English parallel corpus. Training 269\n",
      "set size 276K. Source and Target vocabulary 270\n",
      "size is 30K each. 271\n",
      "•Helsinki-NLP/opus-100 en-hi subset : 272\n",
      "Engish-Hindi parallel corpus. Training set 273\n",
      "size 534K. Source and Target vocabulary size 274\n",
      "is 40K each. 275\n",
      "•Helsinki-NLP/opus-100 de-en subset : 276\n",
      "German-English parallel corpus. Training set 277\n",
      "size 1M. Source and Target vocabulary size is 278\n",
      "30K each. 279\n",
      "3Dataset #EpochsBLEU Score Running Time\n",
      "SOTA ( µ±σ) Our ( µ±σ) Improvement SOTA Our Improvement\n",
      "en-fr 20 30.07±0.41 31 .29±0.15 4 .05% 6.10 6 .60 8 .19%\n",
      "af-en 20 31.06±0.39 34 .90±0.42 12 .36% 5.78 6 .60 14 .10%\n",
      "en-hi 11 10.98±0.24 12 .21±0.11 11 .20% 10.78 11 .40 5 .75%\n",
      "de-en 6 16.09±0.75 16 .77±0.43 4 .22% 9.63 10 .20 5 .91%\n",
      "Table 1: Comparison of BLEU score on different datasets.\n",
      "DatasetSOTA # Epochs Running Time\n",
      "BLEU SOTA Our Improvement SOTA Our Improvement\n",
      "en-fr 30.07 20 18 10.00% 6.10 6 .00 1 .64%\n",
      "af-en 31.06 20 12 40.00% 5.78 4 .00 30 .79%\n",
      "en-hi 10.98 11 7 36.36% 10.78 7 .31 32 .18%\n",
      "de-en 16.09 6 5 16.66% 9.63 9 .00 6 .54%\n",
      "Table 2: Comparison of training time to achieve SOTA results on different datasets.\n",
      "6 Experimental Results 280\n",
      "Experimental Setup 281\n",
      "The datasets are accessed form Hugging Face 282\n",
      "dataset. We have used the Tokenizers library from 283\n",
      "Hugging Face for pre-processing, normalization, 284\n",
      "and WordPiece tokenizer creation. The vocabulary 285\n",
      "size for each dataset is mentioned in §5. To han- 286\n",
      "dle padding for dynamic batching we have used 287\n",
      "the DataCollatorForSeq2Seq library from Hugging 288\n",
      "Face. To implement model architecture we have 289\n",
      "used the Transformer module from PyTorch. Fi- 290\n",
      "nally, for evaluation, we have used the SacreBLEU 291\n",
      "library from Hugging Face. For detailed model 292\n",
      "configuration, hyperparameters, and parameters for 293\n",
      "packages, please see Appendix B. 294\n",
      "Results 295\n",
      "All the models are trained on P100 GPU for a mini- 296\n",
      "mum of 12 hours or 20 epochs, whichever is lower. 297\n",
      "All the results reported are averages of 3 indepen- 298\n",
      "dent runs of each experiment. For all independent 299\n",
      "runs of the dataset, the training and validation set 300\n",
      "is kept the same. For all the datasets the validation 301\n",
      "set size is 2000. There is no overlap between the 302\n",
      "training and validation datasets. 303\n",
      "For all the experiments, we have used Adam op- 304\n",
      "timizer with a constant learning rate of 2e-5. Since 305\n",
      "for our proposed method, we have a separate loss 306\n",
      "function for the Encoder training, so we used a sep- 307\n",
      "arate Adam optimizer but with the same learning 308\n",
      "rate. All the models are trained from scratch with 309Xavier uniform initialization of parameter weights. 310\n",
      "We have set the maximum sequence length to 128 311\n",
      "for both the Encoder and Decoder blocks for all the 312\n",
      "datasets. We have used greedy decoding strategy 313\n",
      "for inference. 314\n",
      "Table 1 shows the comparison of BLEU score 315\n",
      "and training time between the state-of-the-art 316\n",
      "(SOTA) and our method. From the result, we can 317\n",
      "see that there is a substantial improvement in BLEU 318\n",
      "score at the cost of considerably lower extra time. 319\n",
      "Our proposed method also shows rapid improve- 320\n",
      "ment in Neural Machine Translation (NMT) model 321\n",
      "performance. Table 2 shows the comparison of 322\n",
      "training time and number of epochs between SOTA 323\n",
      "and our method to reach SOTA performance. The 324\n",
      "epoch-to-epoch improvement in performance as 325\n",
      "measured by the BLEU score is much more than 326\n",
      "that of SOTA. Hence, by training in our proposed 327\n",
      "method we reach the SOTA results at a lower train- 328\n",
      "ing time. 329\n",
      "7 Conclusions 330\n",
      "In this work, we show that by integrating the En- 331\n",
      "coder training steps we can achieve improved per- 332\n",
      "formance of the NMT model over baseline in low- 333\n",
      "resource settings. Our procedure shows faster con- 334\n",
      "vergence to baseline results. The proposed con- 335\n",
      "cept can be easily implemented with very little 336\n",
      "change to the training procedure and model archi- 337\n",
      "tecture. The only extra space required in the pro- 338\n",
      "posed method is in the implementation extra head 339\n",
      "(linear layer) after the Encoder as defined in §4. 340\n",
      "4Limitations 341\n",
      "We have conducted experiments only on the 342\n",
      "Encoder-Decoder model from (Vaswani et al., 343\n",
      "2017). This is a supervised training setup with 344\n",
      "separate embedding layers for the Encoder and 345\n",
      "Decoder block. The model is also trained as a 346\n",
      "bilingual model where we have only one source 347\n",
      "language and one target language. Our proposed 348\n",
      "method shows improvement in performance at low- 349\n",
      "resource settings and the results may not extrapo- 350\n",
      "late to high resource settings. 351\n",
      "We have not tested the efficiency of this approach 352\n",
      "on larger and multilingual models such as T5 from 353\n",
      "Google (Raffel et al., 2019). 354\n",
      "Ethics Statement 355\n",
      "All the datasets used for our experiments are open 356\n",
      "sourced and proper citations for data sources are 357\n",
      "provided where required. Thus, we do not see any 358\n",
      "ethical concern in the paper. 359\n",
      "References 360\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and 361\n",
      "Kristina Toutanova. 2018. BERT: pre-training of 362\n",
      "deep bidirectional transformers for language under- 363\n",
      "standing. CoRR , abs/1810.04805. 364\n",
      "Junliang Guo, Zhirui Zhang, Linli Xu, Hao-Ran Wei, 365\n",
      "Boxing Chen, and Enhong Chen. 2020. Incorpo- 366\n",
      "rating BERT into parallel sequence decoding with 367\n",
      "adapters. CoRR , abs/2010.06138. 368\n",
      "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan 369\n",
      "Ghazvininejad, Abdelrahman Mohamed, Omer Levy, 370\n",
      "Veselin Stoyanov, and Luke Zettlemoyer. 2019. 371\n",
      "BART: denoising sequence-to-sequence pre-training 372\n",
      "for natural language generation, translation, and com- 373\n",
      "prehension. CoRR , abs/1910.13461. 374\n",
      "Zehui Lin, Xiao Pan, Mingxuan Wang, Xipeng 375\n",
      "Qiu, Jiangtao Feng, Hao Zhou, and Lei Li. 2020. 376\n",
      "Pre-training multilingual neural machine transla- 377\n",
      "tion by leveraging alignment information. CoRR , 378\n",
      "abs/2010.03142. 379\n",
      "Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey 380\n",
      "Edunov, Marjan Ghazvininejad, Mike Lewis, and 381\n",
      "Luke Zettlemoyer. 2020. Multilingual denoising 382\n",
      "pre-training for neural machine translation. CoRR , 383\n",
      "abs/2001.08210. 384\n",
      "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine 385\n",
      "Lee, Sharan Narang, Michael Matena, Yanqi Zhou, 386\n",
      "Wei Li, and Peter J. Liu. 2019. Exploring the limits 387\n",
      "of transfer learning with a unified text-to-text trans- 388\n",
      "former. CoRR , abs/1910.10683. 389Shuo Ren, Long Zhou, Shujie Liu, Furu Wei, Ming 390\n",
      "Zhou, and Shuai Ma. 2021. Semface: Pre-training 391\n",
      "encoder and decoder with a semantic interface for 392\n",
      "neural machine translation. In Annual Meeting of the 393\n",
      "Association for Computational Linguistics . 394\n",
      "Jörg Tiedemann. 2012. Parallel data, tools and inter- 395\n",
      "faces in OPUS. In Proceedings of the Eighth In- 396\n",
      "ternational Conference on Language Resources and 397\n",
      "Evaluation (LREC’12) , pages 2214–2218, Istanbul, 398\n",
      "Turkey. European Language Resources Association 399\n",
      "(ELRA). 400\n",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob 401\n",
      "Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz 402\n",
      "Kaiser, and Illia Polosukhin. 2017. Attention is all 403\n",
      "you need. CoRR , abs/1706.03762. 404\n",
      "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V . Le, 405\n",
      "Mohammad Norouzi, Wolfgang Macherey, Maxim 406\n",
      "Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff 407\n",
      "Klingner, Apurva Shah, Melvin Johnson, Xiaobing 408\n",
      "Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, 409\n",
      "Taku Kudo, Hideto Kazawa, Keith Stevens, George 410\n",
      "Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason 411\n",
      "Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, 412\n",
      "Greg Corrado, Macduff Hughes, and Jeffrey Dean. 413\n",
      "2016. Google’s neural machine translation system: 414\n",
      "Bridging the gap between human and machine trans- 415\n",
      "lation. CoRR , abs/1609.08144. 416\n",
      "Shuoheng Yang, Yuxin Wang, and Xiaowen Chu. 2020. 417\n",
      "A survey of deep learning techniques for neural ma- 418\n",
      "chine translation. CoRR , abs/2002.07526. 419\n",
      "Biao Zhang, Philip Williams, Ivan Titov, and Rico Sen- 420\n",
      "nrich. 2020. Improving massively multilingual neu- 421\n",
      "ral machine translation and zero-shot translation. In 422\n",
      "Proceedings of the 58th Annual Meeting of the Asso- 423\n",
      "ciation for Computational Linguistics , pages 1628– 424\n",
      "1639, Online. Association for Computational Linguis- 425\n",
      "tics. 426\n",
      "5A Graphical Representation of Encoder 427\n",
      "Training 428\n",
      "Figure 1 shows the detailed architecture of our pro- 429\n",
      "posed Encoder training procedure on top of a stan- 430\n",
      "dard Transformer model. 431\n",
      "B Parameter Details 432\n",
      "B.1 Model Architecture 433\n",
      "The architecture is based on the paper (Vaswani 434\n",
      "et al., 2017) 435\n",
      "d_model or embedding_dim = 512 436\n",
      "nhead = 8 437\n",
      "num_encoder_layers = 6 438\n",
      "num_decoder_layers = 6 439\n",
      "dim_feedforward = 2048 440\n",
      "optimizer = Adam 441\n",
      "B.2 Hyperparameters 442\n",
      "dropout = 0.1 443\n",
      "optimizer learning rate = 2e-5 444\n",
      "Batch Size = 16 445\n",
      "B.3 Parameters For Packages 446\n",
      "normalizers = BertNormalizer(lowercase=True) 447\n",
      "pre_tokenizers = BertPreTokenizer() 448\n",
      "6Figure 1: Our proposed Encoder Training Architecture on top of base Transformer model. The Transformer image\n",
      "is taken form the paper ‘Attention Is All You Need’ (Vaswani et al., 2017).\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print(documents_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Splitting\n",
    "chunk_size = 512\n",
    "chunk_overlap = 50\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "split = splitter.split_text(documents_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.phi3_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/codespace/.phi3_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(doc):\n",
    "    model_input = doc\n",
    "    out =  model.encode(model_input)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(map(get_embeddings,split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_array = np.array(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Indexes](https://github.com/facebookresearch/faiss/wiki/Faiss-indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "index = faiss.IndexFlatL2(1024)\n",
    "index.add(x_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, 'test.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
