{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/digital_research_guide/phi3_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from pypdf import PdfReader\n",
    "import os\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pre_processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/digital_research_guide/phi3_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/workspaces/digital_research_guide/phi3_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "check_point = 'mixedbread-ai/mxbai-embed-large-v1'\n",
    "embedding_model = SentenceTransformer(check_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processing.model = embedding_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader('/workspaces/digital_research_guide/1706.03762v7.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_list = pre_processing.parese_doc(reader)\n",
    "pre_processing.create_embedding(context_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linking ONXX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime_genai as og"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = og.Model('/workspaces/digital_research_guide/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4')\n",
    "tokenizer = og.Tokenizer(model)\n",
    "tokenizer_stream = tokenizer.create_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = '<|user|>\\n{input} <|end|>\\n<|assistant|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_options ={}\n",
    "#search_options['max_length'] = 4000\n",
    "search_options['temperature'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "index: faiss.IndexFlatL2 = faiss.read_index(os.path.join(base_path, 'doc.index'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "what is transformers With respect to context: Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research. †Work performed while at Google Brain. ‡Work performed while at Google Research. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 20231 Introduction Recurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [ 35,2,5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]. Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [ 21] and conditional computation [ 32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence <|end|>\n",
      "<|assistant|>\n",
      "\n",
      "Output:  The Transformer is a model architecture that revolutionized the field of sequence modeling and transduction tasks, particularly in machine translation. Unlike traditional recurrent neural network (RNN) models, such as LSTM and GRU, which process data sequentially, the Transformer relies entirely on an attention mechanism to draw global dependencies between input and output sequences. This allows for significantly more parallelization during training and inference, leading to faster computation and improved performance.\n",
      "\n",
      "The Transformer consists of an encoder and a decoder, both of which are composed of multiple identical layers. Each layer contains multi-head self-attention and point-wise, fully connected layers. The self-attention mechanism computes the weighted sum of all input positions, allowing the model to capture long-range dependencies between different positions in the sequence. The point-wise layers then apply non-linear transformations to the attention-weighted representations.\n",
      "\n",
      "One of the key advantages of the Transformer is its ability to learn dependencies between distant positions in the input sequence. This is achieved through the use of multi-head attention, which allows the model to attend to multiple positions simultaneously, effectively capturing different aspects of the input sequence.\n",
      "\n",
      "The Transformer has been successfully applied to various sequence modeling and transduction tasks, such as machine translation, text summarization, and speech recognition. It has achieved state-of-the-art results in many of these tasks, outperforming previous RNN-based models.\n",
      "\n",
      "In summary, the Transformer is a powerful model architecture that leverages the attention mechanism to overcome the sequential computation limitations of RNNs, enabling more parallelization and improved performance in sequence modeling and transduction tasks. Its ability to capture long-range dependencies and learn dependencies between distant positions in the input sequence has made it a popular choice in the field of natural language processing and beyond.\n",
      "\n",
      "Error, input cannot be empty\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "        text = input(\"Input: \")\n",
    "        if not text:\n",
    "            print(\"Error, input cannot be empty\")\n",
    "            break\n",
    "\n",
    "        # If there is a chat template, use it\n",
    "        #prompt = f'{chat_template.format(input=text)}'\n",
    "\n",
    "\n",
    "\n",
    "        query_embedding = embedding_model.encode(text).reshape(1, -1)\n",
    "        top_k = 1\n",
    "        _scores, binary_ids = index.search(query_embedding, top_k)\n",
    "        binary_ids = binary_ids[0]\n",
    "        _scores = _scores[0]\n",
    "        temp_list = []\n",
    "        for idx in binary_ids:\n",
    "             temp_list.append(context_list[idx])\n",
    "        context = '. '.join(temp_list)\n",
    "        \n",
    "        text += \" With respect to context: \"+context\n",
    "        \n",
    "\n",
    "        prompt = f'{chat_template.format(input=text)}'\n",
    "        print(prompt)\n",
    "\n",
    "\n",
    "\n",
    "        input_tokens = tokenizer.encode(prompt)\n",
    "\n",
    "        params = og.GeneratorParams(model)\n",
    "        params.try_use_cuda_graph_with_max_batch_size(1)\n",
    "        params.set_search_options(**search_options)\n",
    "        params.input_ids = input_tokens\n",
    "        generator = og.Generator(model, params)\n",
    "        \n",
    "\n",
    "        print()\n",
    "        print(\"Output: \", end='', flush=True)\n",
    "\n",
    "        try:\n",
    "            while not generator.is_done():\n",
    "                generator.compute_logits()\n",
    "                generator.generate_next_token()\n",
    "                new_token = generator.get_next_tokens()[0]\n",
    "                print(tokenizer_stream.decode(new_token), end='', flush=True)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"  --control+c pressed, aborting generation--\")\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".phi3_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
