{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/digital_research_guide/phi3_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from pypdf import PdfReader\n",
    "import os\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pre_processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/digital_research_guide/phi3_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/workspaces/digital_research_guide/phi3_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "check_point = 'mixedbread-ai/mxbai-embed-large-v1'\n",
    "embedding_model = SentenceTransformer(check_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processing.model = embedding_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reader = PdfReader('/workspaces/digital_research_guide/1706.03762v7.pdf')\n",
    "reader = PdfReader('/workspaces/digital_research_guide/Novel_Encoder_Training_for_Neural_Machine_Translation_in_Low_Resource_Settings.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_section = \"Abstract\"\n",
    "ignore_after = \"References\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_list = pre_processing.parese_doc(reader,first_section,ignore_after)\n",
    "pre_processing.create_embedding(context_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linking ONXX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime_genai as og"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = og.Model('/workspaces/digital_research_guide/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4')\n",
    "tokenizer = og.Tokenizer(model)\n",
    "tokenizer_stream = tokenizer.create_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = '<|user|>\\n{input} <|end|>\\n<|assistant|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_options ={}\n",
    "#search_options['max_length'] = 4000\n",
    "search_options['temperature'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "index: faiss.IndexFlatL2 = faiss.read_index(os.path.join(base_path, 'doc.index'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "describe the paper. With respect to context: Source and Target vocabulary size 272 is 40K each. 273 •Helsinki-NLP/opus-100 de-en subset : 274 German-Engish parallel corpus. Training set 275 size 1M. Source and Target vocabulary size is 276 30K each. 277 3Dataset #EpochsBLEU Score Running Time SOTA ( µ±σ) Our ( µ±σ) Improvement SOTA Our Improvement en-fr 20 30.07±0.41 31 .29±0.15 4 .05% 6.10 6 .60 8 .19% af-en 20 31.06±0.39 34 .90±0.42 12 .36% 5.78 6 .60 14 .10% en-hi 11 10.98±0.24 12 .21±0.11 11 .20% 10.78 11 .40 5 .75% de-en 6 16.09±0.75 16 .77±0.43 4 .22% 9.63 10 .20 5 .91% Table 1: Comparison of BLEU score on different datasets. Dataset SOTA BLEU ( µ)# Epochs Running Time SOTA Our Improvement SOTA Our Improvement en-fr 30.07 20 18 10.00% 6.10 6 .00 1 .64% af-en 31.06 20 12 40.00% 5.78 4 .00 30 .79% en-hi 10.98 11 7 36.36% 10.78 7 .31 32 .18% de-en 16.09 6 5 16.66% 9.63 9 .00 6 .54% Table 2: Comparison of training time to achieve SOTA results on different datasets. 6 Experimental Setup 278 The datasets are accessed form Hugging Face 279 dataset. We have used the Tokenizers library from 280 Hugging Face for pre-processing, normalization, 281 and WordPiece tokenizer creation. The vocabulary 282 size for each dataset is mentioned in §5. To han- 283 dle padding for dynamic batching we have used 284 the DataCollatorForSeq2Seq library from Hugging 285 Face. To implement model architecture we have 286 used the Transformer module from PyTorch. Fi- 287 nally, for evaluation, we have used the SacreBLEU 288 library from Hugging Face. For detailed model con- 289 figuration See Appendix A and hyperparameters 290 are mentioned in Appendix B,Appendix C 291 7 Results 292 All the models are trained on P100 GPU for a mini- 293 mum of 12 hours or 20 epochs, whichever is lower. 294 All the results reported are averages of 3 indepen- 295 dent runs of each experiment to take into account 296 the run-to-run variation. For all independent runs 297 of the dataset, the training and validation set is kept 298 the same. For all the datasets the validation set size 299 is 2K. There is no overlap between the training and 300 validation datasets. 301 For all the experiments we have used Adam op- 302 timizer with a constant learning rate of 2e-5. For 303 our proposed method we have a separate loss func- 304 tion for the Encoder training so we used a separate 305 Adam optimizer with the same learning rate. All 306 the models are trained from scratch with Xavier 307uniform distribution of parameter weights <|end|>\n",
      "<|assistant|>\n",
      "\n",
      "Output:  This paper presents a study on the performance of a Transformer-based model for machine translation tasks across various language pairs. The datasets used include the Helsinki-NLP/opus-100 de-en subset, a German-English parallel corpus, and other language pairs such as af-en, en-hi, and de-en. The source and target vocabulary sizes for the de-en dataset are 272 and 40K respectively, while for the other datasets, they are 273 and 30K each. The training set consists of 1 million tokens.\n",
      "\n",
      "The proposed method utilizes a WordPiece tokenizer and the Transformer module from PyTorch. The model is trained on a P100 GPU for a mini-batch size of 12 hours or 20 epochs, whichever is lower. The results are evaluated using the SacreBLEU library, and the BLEU scores for different datasets are reported.\n",
      "\n",
      "Table 1 presents the BLEU scores and the number of epochs required to achieve the State of the Art (SOTA) results for each dataset. The table also includes the improvement achieved by the proposed method over the SOTA results.\n",
      "\n",
      "Table 2 shows the training time required to achieve SOTA results for each dataset.\n",
      "\n",
      "The experimental setup includes the use of the Tokenizers library from Hugging Face for pre-processing and normalization, and the DataCollatorForSeq2Seq library for dynamic batching. The hyperparameters and model configurations are detailed in the appendices.\n",
      "\n",
      "The results demonstrate the effectiveness of the proposed method in improving translation quality across different language pairs. The BLEU scores and training times for the various datasets are summarized in the tables, showcasing the model's performance and efficiency.\n",
      "\n",
      "<|user|>\n",
      "what is encoder head? With respect to context: 168 4 Encoder Training 169 Before we dive into the training procedure for En- 170 coder let us first describe the inputs to Encoder and 171 outputs from Encoder in detail. The input to the 172 Encoder block is the tokenized source language 173 sentence. Let’s take the input size as seq_len where 174 seq_len is the number of tokens that are input to the 175 Encoder block. Due to the multi-head attention ar- 176 chitecture of the Encoder, the length of output will 177 be the same as the length of input. This is the rep- 178 resentation of the input in a predefined embedding 179 2dimension space which is 512 in our case. Thus the 180 Encoder output size is (seq_len,embedding_dim). 181 This representation goes as input to the Decoder 182 block to generate the target language tokens. The 183 intuition is that a good feature representation of 184 input tokens in a different embedding dimension 185 should also be able to produce back the original 186 input tokens along with aiding the Decoder block 187 to produce target tokens. 188 We have added a head (encoder_head) compris- 189 ing a linear layer with an input dimension equal 190 to the embedding dimension and an output size 191 equal to the source vocabulary size. The Encoder 192 Block output is passed through this layer to cre- 193 ate a probability distribution of source vocabulary 194 tokens for the entire sequence of Encoder output. 195 The Encoder is trained with the objective to re- 196 produce correct source tokens from the Encoder 197 output which is also the input to the Decoder block. 198 Thus now, we have a different loss function for the 199 Encoder block. It is easy to train the Encoder with 200 the above objective. Along with training the En- 201 coder separately as described above we also train 202 the entire Encoder-Decoder block as a single end- 203 to-end system with NMT objective as described in 204 §3. The training objective for the Encoder alone 205 does not impart any inherent meaning but when 206 used in conjunction with the end-to-end training 207 objective of the NMT model, it helps in optimiz- 208 ing the Encoder for better feature representation in 209 translation tasks. This encoder_head is used only 210 during training. This layer is not involved in the 211 inference. 212 The above training objective for the Encoder 213 block introduces two new problems. Training the 214 Encoder block separately with different objectives 215 will introduce extra computation hence increasing 216 the training time. If we keep training the Encoder 217 at every training step with the above-mentioned 218 objective then initially model performance may 219 improve but gradually it will subside as the Encoder 220 training objective is to optimize the Encoder output 221 to re-produce the Encoder input tokens and not the 222 target tokens <|end|>\n",
      "<|assistant|>\n",
      "\n",
      "Output: The encoder head, in the context of neural machine translation (NMT), refers to a specific component within the encoder block that is responsible for generating a probability distribution over the source vocabulary from the encoded input sequence. This is achieved by applying a linear layer with a dimensionality equal to the embedding dimension of the input tokens and an output size equal to the source vocabulary size. The encoder head is crucial for the training process as it helps the model learn to produce the correct source tokens from the encoded output, which is also the input to the decoder block. Although the encoder head is used only during training and not in inference, it plays a significant role in optimizing the encoder for better feature representation, which is essential for translation tasks. However, it's important to note that training the encoder separately with different objectives can introduce extra computation and potentially affect the model's performance over time.\n",
      "\n",
      "<|user|>\n",
      "describe encoder training process. With respect to context: 168 4 Encoder Training 169 Before we dive into the training procedure for En- 170 coder let us first describe the inputs to Encoder and 171 outputs from Encoder in detail. The input to the 172 Encoder block is the tokenized source language 173 sentence. Let’s take the input size as seq_len where 174 seq_len is the number of tokens that are input to the 175 Encoder block. Due to the multi-head attention ar- 176 chitecture of the Encoder, the length of output will 177 be the same as the length of input. This is the rep- 178 resentation of the input in a predefined embedding 179 2dimension space which is 512 in our case. Thus the 180 Encoder output size is (seq_len,embedding_dim). 181 This representation goes as input to the Decoder 182 block to generate the target language tokens. The 183 intuition is that a good feature representation of 184 input tokens in a different embedding dimension 185 should also be able to produce back the original 186 input tokens along with aiding the Decoder block 187 to produce target tokens. 188 We have added a head (encoder_head) compris- 189 ing a linear layer with an input dimension equal 190 to the embedding dimension and an output size 191 equal to the source vocabulary size. The Encoder 192 Block output is passed through this layer to cre- 193 ate a probability distribution of source vocabulary 194 tokens for the entire sequence of Encoder output. 195 The Encoder is trained with the objective to re- 196 produce correct source tokens from the Encoder 197 output which is also the input to the Decoder block. 198 Thus now, we have a different loss function for the 199 Encoder block. It is easy to train the Encoder with 200 the above objective. Along with training the En- 201 coder separately as described above we also train 202 the entire Encoder-Decoder block as a single end- 203 to-end system with NMT objective as described in 204 §3. The training objective for the Encoder alone 205 does not impart any inherent meaning but when 206 used in conjunction with the end-to-end training 207 objective of the NMT model, it helps in optimiz- 208 ing the Encoder for better feature representation in 209 translation tasks. This encoder_head is used only 210 during training. This layer is not involved in the 211 inference. 212 The above training objective for the Encoder 213 block introduces two new problems. Training the 214 Encoder block separately with different objectives 215 will introduce extra computation hence increasing 216 the training time. If we keep training the Encoder 217 at every training step with the above-mentioned 218 objective then initially model performance may 219 improve but gradually it will subside as the Encoder 220 training objective is to optimize the Encoder output 221 to re-produce the Encoder input tokens and not the 222 target tokens <|end|>\n",
      "<|assistant|>\n",
      "\n",
      "Output: The training process for an encoder in a neural machine translation (NMT) system involves several key steps and considerations. Here's a detailed description of the encoder training process:\n",
      "\n",
      "\n",
      "1. **Input Preparation**: The encoder receives tokenized input sentences in the source language. Each token is represented by a sequence length (seq_len), which is the number of tokens in the input sentence.\n",
      "\n",
      "\n",
      "2. **Embedding**: The input tokens are transformed into a fixed-size, predefined embedding space (e.g., 512 dimensions). This embedding space is shared across all tokens and is learned during training.\n",
      "\n",
      "\n",
      "3. **Encoder Architecture**: The encoder employs a multi-head attention mechanism, which allows it to capture different aspects of the input sequence. The output of the encoder is a sequence of the same length as the input, with each element representing the encoded features of the corresponding input token.\n",
      "\n",
      "\n",
      "4. **Feature Representation**: The encoder's output is a feature representation of the input tokens. This representation is intended to capture the contextual information of the source sentence, which will be useful for the decoder to generate the target language tokens.\n",
      "\n",
      "\n",
      "5. **Head for Source Vocabulary**: An additional head (encoder_head) is added to the encoder, consisting of a linear layer with an input dimension equal to the embedding dimension and an output size equal to the source vocabulary size. This head is used to create a probability distribution over the source vocabulary for the entire sequence.\n",
      "\n",
      "\n",
      "6. **Training Objective**: The encoder is trained with the objective of producing correct source tokens from its output, which serves as the input to the decoder. This training objective is different from the end-to-end NMT objective and is used to optimize the encoder's feature representation.\n",
      "\n",
      "\n",
      "7. **Separate Training**: The encoder is trained separately with the aforementioned objective, which may introduce extra computation and increase training time. However, this approach helps in optimizing the encoder for better feature representation in translation tasks.\n",
      "\n",
      "\n",
      "8. **End-to-End Training**: Despite the separate training of the encoder, it is also trained as part of the entire Encoder-Decoder system with the NMT objective. This ensures that the encoder's feature representation is suitable for the overall translation task.\n",
      "\n",
      "\n",
      "9. **Problems with Separate Training**: Training the encoder separately with different objectives can lead to extra computation and potentially suboptimal performance over time. However, it can initially improve model performance by focusing on the encoder's ability to recreate the input tokens.\n",
      "\n",
      "\n",
      "In summary, the encoder training process involves embedding the source language tokens, using a multi-head attention mechanism to capture contextual information, and training with an objective that optimizes the encoder's output to recreate the input tokens. This training is done separately but is also integrated into the end-to-end NMT training process.\n",
      "\n",
      "<|user|>\n",
      "what are the improvements mentioned in the paper? With respect to context: Source and Target vocabulary size 272 is 40K each. 273 •Helsinki-NLP/opus-100 de-en subset : 274 German-Engish parallel corpus. Training set 275 size 1M. Source and Target vocabulary size is 276 30K each. 277 3Dataset #EpochsBLEU Score Running Time SOTA ( µ±σ) Our ( µ±σ) Improvement SOTA Our Improvement en-fr 20 30.07±0.41 31 .29±0.15 4 .05% 6.10 6 .60 8 .19% af-en 20 31.06±0.39 34 .90±0.42 12 .36% 5.78 6 .60 14 .10% en-hi 11 10.98±0.24 12 .21±0.11 11 .20% 10.78 11 .40 5 .75% de-en 6 16.09±0.75 16 .77±0.43 4 .22% 9.63 10 .20 5 .91% Table 1: Comparison of BLEU score on different datasets. Dataset SOTA BLEU ( µ)# Epochs Running Time SOTA Our Improvement SOTA Our Improvement en-fr 30.07 20 18 10.00% 6.10 6 .00 1 .64% af-en 31.06 20 12 40.00% 5.78 4 .00 30 .79% en-hi 10.98 11 7 36.36% 10.78 7 .31 32 .18% de-en 16.09 6 5 16.66% 9.63 9 .00 6 .54% Table 2: Comparison of training time to achieve SOTA results on different datasets. 6 Experimental Setup 278 The datasets are accessed form Hugging Face 279 dataset. We have used the Tokenizers library from 280 Hugging Face for pre-processing, normalization, 281 and WordPiece tokenizer creation. The vocabulary 282 size for each dataset is mentioned in §5. To han- 283 dle padding for dynamic batching we have used 284 the DataCollatorForSeq2Seq library from Hugging 285 Face. To implement model architecture we have 286 used the Transformer module from PyTorch. Fi- 287 nally, for evaluation, we have used the SacreBLEU 288 library from Hugging Face. For detailed model con- 289 figuration See Appendix A and hyperparameters 290 are mentioned in Appendix B,Appendix C 291 7 Results 292 All the models are trained on P100 GPU for a mini- 293 mum of 12 hours or 20 epochs, whichever is lower. 294 All the results reported are averages of 3 indepen- 295 dent runs of each experiment to take into account 296 the run-to-run variation. For all independent runs 297 of the dataset, the training and validation set is kept 298 the same. For all the datasets the validation set size 299 is 2K. There is no overlap between the training and 300 validation datasets. 301 For all the experiments we have used Adam op- 302 timizer with a constant learning rate of 2e-5. For 303 our proposed method we have a separate loss func- 304 tion for the Encoder training so we used a separate 305 Adam optimizer with the same learning rate. All 306 the models are trained from scratch with Xavier 307uniform distribution of parameter weights <|end|>\n",
      "<|assistant|>\n",
      "\n",
      "Output: The paper discusses several improvements in the context of training Transformer models for machine translation tasks. The key points mentioned in the paper are:\n",
      "\n",
      "\n",
      "1. Dataset Preparation:\n",
      "\n",
      "   - The paper uses the Helsinki-NLP/opus-100 de-en subset, which is a German-English parallel corpus.\n",
      "\n",
      "   - The training set size is 1 million tokens, with a source and target vocabulary size of 30,000 tokens each.\n",
      "\n",
      "   - For the 3Dataset, the German-English parallel corpus is used, with a vocabulary size of 30,000 tokens each.\n",
      "\n",
      "\n",
      "2. Model Training:\n",
      "\n",
      "   - The models are trained on a P100 GPU for a mini-mum of 12 hours or 20 epochs, whichever is lower.\n",
      "\n",
      "   - The training and validation set sizes are 2,000 tokens each, with no overlap between the two datasets.\n",
      "\n",
      "   - The Adam optimizer with a learning rate of 2e-5 is used for all experiments.\n",
      "\n",
      "\n",
      "3. Evaluation and Results:\n",
      "\n",
      "   - The BLEU score is used as the evaluation metric, with the following results reported:\n",
      "\n",
      "     - For the de-en dataset, the SOTA (State of the Art) BLEU score is 16.09 with a standard deviation of ±0.75.\n",
      "\n",
      "     - The SOTA BLEU score for the en-fr dataset is 6.10 with a standard deviation of ±0.41.\n",
      "\n",
      "     - The SOTA BLEU score for the af-en dataset is 5.78 with a standard deviation of ±0.15.\n",
      "\n",
      "     - The SOTA BLEU score for the en-hi dataset is 10.78 with a standard deviation of ±0.24.\n",
      "\n",
      "     - The SOTA BLEU score for the de-en dataset is 9.63 with a standard deviation of ±0.5.\n",
      "\n",
      "\n",
      "4. Improvements:\n",
      "\n",
      "   - The paper reports improvements over the SOTA results for the en-fr, af-en, and de-en datasets.\n",
      "\n",
      "   - The improvements are as follows:\n",
      "\n",
      "     - en-fr: 4.00% improvement\n",
      "\n",
      "     - af-en: 30.00% improvement\n",
      "\n",
      "     - de-en: 16.66% improvement\n",
      "\n",
      "\n",
      "5. Experimental Setup:\n",
      "\n",
      "   - The datasets are accessed from Hugging Face.\n",
      "\n",
      "   - The Tokenizers library from Hugging Face is used for pre-processing and normalization.\n",
      "\n",
      "   - The WordPiece tokenizer is created using the Hugging Face library.\n",
      "\n",
      "   - The DataCollatorForSeq2Seq library from Hugging Face is used for dynamic batching.\n",
      "\n",
      "   - The Transformer module from PyTorch is used for the model architecture.\n",
      "\n",
      "   - SacreBLEU library from Hugging Face is used for evaluation.\n",
      "\n",
      "\n",
      "The paper provides a comprehensive analysis of the dataset sizes, model training, and evaluation metrics, along with the reported improvements over the SOTA results.\n",
      "\n",
      "Error, input cannot be empty\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "        text = input(\"Input: \")\n",
    "        if not text:\n",
    "            print(\"Error, input cannot be empty\")\n",
    "            break\n",
    "\n",
    "        # If there is a chat template, use it\n",
    "        #prompt = f'{chat_template.format(input=text)}'\n",
    "\n",
    "\n",
    "\n",
    "        query_embedding = embedding_model.encode(text).reshape(1, -1)\n",
    "        top_k = 1\n",
    "        _scores, binary_ids = index.search(query_embedding, top_k)\n",
    "        binary_ids = binary_ids[0]\n",
    "        _scores = _scores[0]\n",
    "        temp_list = []\n",
    "        for idx in binary_ids:\n",
    "             temp_list.append(context_list[idx])\n",
    "        context = '. '.join(temp_list)\n",
    "        \n",
    "        text += \" With respect to context: \"+context\n",
    "        \n",
    "\n",
    "        prompt = f'{chat_template.format(input=text)}'\n",
    "        print(prompt)\n",
    "\n",
    "\n",
    "\n",
    "        input_tokens = tokenizer.encode(prompt)\n",
    "\n",
    "        params = og.GeneratorParams(model)\n",
    "        params.try_use_cuda_graph_with_max_batch_size(1)\n",
    "        params.set_search_options(**search_options)\n",
    "        params.input_ids = input_tokens\n",
    "        generator = og.Generator(model, params)\n",
    "        \n",
    "\n",
    "        print()\n",
    "        print(\"Output: \", end='', flush=True)\n",
    "\n",
    "        try:\n",
    "            while not generator.is_done():\n",
    "                generator.compute_logits()\n",
    "                generator.generate_next_token()\n",
    "                new_token = generator.get_next_tokens()[0]\n",
    "                print(tokenizer_stream.decode(new_token), end='', flush=True)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"  --control+c pressed, aborting generation--\")\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".phi3_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
