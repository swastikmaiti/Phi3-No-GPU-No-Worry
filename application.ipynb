{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/digital_research_guide/phi3_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from pypdf import PdfReader\n",
    "import os\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pre_processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/digital_research_guide/phi3_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/workspaces/digital_research_guide/phi3_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "check_point = 'mixedbread-ai/mxbai-embed-large-v1'\n",
    "embedding_model = SentenceTransformer(check_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processing.model = embedding_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader('/workspaces/digital_research_guide/1706.03762v7.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_section = \"Abstract\"\n",
    "ignore_after = \"References\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_list = pre_processing.parese_doc(reader,first_section,ignore_after)\n",
    "pre_processing.create_embedding(context_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linking ONXX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime_genai as og"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = og.Model('/workspaces/digital_research_guide/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4')\n",
    "tokenizer = og.Tokenizer(model)\n",
    "tokenizer_stream = tokenizer.create_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = '<|user|>\\n{input} <|end|>\\n<|assistant|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_options ={}\n",
    "#search_options['max_length'] = 4000\n",
    "search_options['temperature'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "index: faiss.IndexFlatL2 = faiss.read_index(os.path.join(base_path, 'doc.index'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "        text = input(\"Input: \")\n",
    "        if not text:\n",
    "            print(\"Error, input cannot be empty\")\n",
    "            break\n",
    "\n",
    "        # If there is a chat template, use it\n",
    "        #prompt = f'{chat_template.format(input=text)}'\n",
    "\n",
    "\n",
    "\n",
    "        query_embedding = embedding_model.encode(text).reshape(1, -1)\n",
    "        top_k = 1\n",
    "        _scores, binary_ids = index.search(query_embedding, top_k)\n",
    "        binary_ids = binary_ids[0]\n",
    "        _scores = _scores[0]\n",
    "        temp_list = []\n",
    "        for idx in binary_ids:\n",
    "             temp_list.append(context_list[idx])\n",
    "        context = '. '.join(temp_list)\n",
    "        \n",
    "        text += \" With respect to context: \"+context\n",
    "        \n",
    "\n",
    "        prompt = f'{chat_template.format(input=text)}'\n",
    "        print(prompt)\n",
    "\n",
    "\n",
    "\n",
    "        input_tokens = tokenizer.encode(prompt)\n",
    "\n",
    "        params = og.GeneratorParams(model)\n",
    "        params.try_use_cuda_graph_with_max_batch_size(1)\n",
    "        params.set_search_options(**search_options)\n",
    "        params.input_ids = input_tokens\n",
    "        generator = og.Generator(model, params)\n",
    "        \n",
    "\n",
    "        print()\n",
    "        print(\"Output: \", end='', flush=True)\n",
    "\n",
    "        try:\n",
    "            while not generator.is_done():\n",
    "                generator.compute_logits()\n",
    "                generator.generate_next_token()\n",
    "                new_token = generator.get_next_tokens()[0]\n",
    "                print(tokenizer_stream.decode(new_token), end='', flush=True)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"  --control+c pressed, aborting generation--\")\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".phi3_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
